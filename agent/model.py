import requests

# Default model to use with Ollama
MODEL_NAME = "qwen2.5:14b"

# Ollama API configuration
OLLAMA_HOST = "localhost"
OLLAMA_PORT = 11434  # Default Ollama port

def query_model(prompt: str, model: str = MODEL_NAME) -> str:
    """
    Sends a prompt to the local Ollama server and returns the generated response.

    Args:
        prompt (str): The user prompt to send to the language model.
        model (str): The model name to use (default: qwen2.5:14b).

    Returns:
        str: The response text generated by the model.

    Raises:
        RuntimeError: If the request to Ollama fails (e.g. server not running).
    """
    try:
        response = requests.post(
            f"http://{OLLAMA_HOST}:{OLLAMA_PORT}/api/generate",
            json={
                "model": model,       # Which model to run (e.g. "qwen2.5:14b")
                "prompt": prompt,     # The input string to process
                "stream": False       # Disable streaming â€“ get full result at once
            }
        )
        response.raise_for_status()  # Raises HTTPError for 4xx/5xx responses
        return response.json()["response"]  # Extracts actual model reply
    except requests.exceptions.RequestException as e:
        # Catch all client-level network/connection failures and wrap them
        raise RuntimeError(
            f"[ERROR] Could not connect to Ollama at http://{OLLAMA_HOST}:{OLLAMA_PORT}. Is the server running?"
        ) from e
